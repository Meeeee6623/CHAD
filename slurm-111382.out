[2025-04-20 14:08:19,845] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 14:08:30,578] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/pl217/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-04-20 14:08:33,659] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-04-20 14:08:33,659] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
--- Configuration ---
model_name: meta-llama/Llama-3.2-3B-Instruct
model_dtype: bfloat16
attn_implementation: sdpa
emb_dim: 3072
n_layers: 28
n_heads: 24
n_kv_groups: 8
hidden_dim: 8192
rope_base: 500000.0
rms_norm_eps: 1e-5
vocab_size: 128256
original_context_length: 128000
context_length: 4096
rope_freq: None
hopfield_heads: 24
hopfield_memory_slots: 256
hopfield_num_updates: 1
hopfield_init_method: embedding_sampling
hopfield_update_strategy: gated
hopfield_memory_update_lr: 0.01
hopfield_gate_input_pooling: mean
hopfield_update_target_method: avg_query
hopfield_clamp_patterns: 5.0
hopfield_clamp_beta: 10.0
hopfield_combine_method: add
dataset_name: deepmind/narrativeqa
story_chunk_size: 768
story_chunk_overlap: 128
max_answer_length: 64
data_cache_dir: ./data_cache
output_dir: ./narrativeqa_hat_finetune_3.2_3B_run1
num_train_epochs: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 2e-05
hopfield_lr_multiplier: 5.0
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
warmup_ratio: 0.05
logging_steps: 20
save_steps: 500
eval_steps: 500
seed: 42
gradient_checkpointing: True
use_lora: True
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
num_beams: 1
---------------------
Loading tokenizer...
No PAD token found. Adding and using EOS token as PAD.
Tokenizer loaded. Vocab size: 128000
BOS: '<|begin_of_text|>' (ID: 128000), EOS: '<|eot_id|>' (ID: 128009), PAD: '<|eot_id|>' (ID: 128009)
Creating/Loading model and weights...
Initializing HopfieldLlama3Model (Scratch Base) with dtype: torch.bfloat16
Initialized Gated Update mechanism.
Gradient checkpointing enabled for Transformer blocks.
Loading pretrained weights for meta-llama/Llama-3.2-3B-Instruct...
Assuming 2 weight shards for meta-llama/Llama-3.2-3B-Instruct.
Loading weights from model-00001-of-00002.safetensors to CPU...
Loading weights from model-00002-of-00002.safetensors to CPU...
Base model weights loaded into HAT structure.
Applying weight tying for output head.
Explicitly initializing Hopfield memory...
Initializing Hopfield memory (embedding_sampling)...
Applying LoRA adapters...
Applying LoRA to modules: ['trf_blocks.23.ff.fc1', 'trf_blocks.0.att.k_proj', 'trf_blocks.0.att.v_proj', 'trf_blocks.26.ff.fc3', 'trf_blocks.9.att.o_proj', 'trf_blocks.5.ff.fc1', 'trf_blocks.7.ff.fc3', 'trf_blocks.13.ff.fc2', 'trf_blocks.10.att.o_proj', 'trf_blocks.18.ff.fc1', 'trf_blocks.2.ff.fc3', 'trf_blocks.13.att.v_proj', 'trf_blocks.16.att.q_proj', 'trf_blocks.8.ff.fc1', 'trf_blocks.1.ff.fc3', 'trf_blocks.16.ff.fc3', 'trf_blocks.17.att.q_proj', 'trf_blocks.4.ff.fc2', 'trf_blocks.25.ff.fc3', 'trf_blocks.20.att.v_proj', 'trf_blocks.3.ff.fc1', 'trf_blocks.7.att.v_proj', 'trf_blocks.21.ff.fc2', 'trf_blocks.0.ff.fc1', 'trf_blocks.23.att.o_proj', 'trf_blocks.4.ff.fc3', 'trf_blocks.9.ff.fc3', 'trf_blocks.25.att.q_proj', 'trf_blocks.23.ff.fc2', 'trf_blocks.3.ff.fc2', 'trf_blocks.0.att.o_proj', 'trf_blocks.16.ff.fc1', 'trf_blocks.27.ff.fc2', 'trf_blocks.12.ff.fc1', 'trf_blocks.14.att.q_proj', 'trf_blocks.10.att.q_proj', 'trf_blocks.2.ff.fc1', 'trf_blocks.10.att.k_proj', 'trf_blocks.24.att.q_proj', 'trf_blocks.2.att.k_proj', 'trf_blocks.1.att.q_proj', 'trf_blocks.27.att.o_proj', 'trf_blocks.22.ff.fc2', 'trf_blocks.15.ff.fc2', 'trf_blocks.15.ff.fc3', 'trf_blocks.21.att.o_proj', 'trf_blocks.13.att.q_proj', 'trf_blocks.19.att.v_proj', 'trf_blocks.23.att.q_proj', 'trf_blocks.19.att.o_proj', 'trf_blocks.3.att.k_proj', 'trf_blocks.27.att.q_proj', 'trf_blocks.12.att.q_proj', 'trf_blocks.8.att.q_proj', 'trf_blocks.24.att.v_proj', 'trf_blocks.4.att.q_proj', 'trf_blocks.26.att.k_proj', 'trf_blocks.5.att.k_proj', 'trf_blocks.13.att.k_proj', 'trf_blocks.13.att.o_proj', 'trf_blocks.23.att.v_proj', 'trf_blocks.25.ff.fc1', 'trf_blocks.16.ff.fc2', 'trf_blocks.6.att.q_proj', 'trf_blocks.8.att.v_proj', 'trf_blocks.24.ff.fc1', 'trf_blocks.5.att.o_proj', 'trf_blocks.4.att.v_proj', 'trf_blocks.17.att.v_proj', 'trf_blocks.7.ff.fc2', 'trf_blocks.11.ff.fc1', 'trf_blocks.14.ff.fc1', 'trf_blocks.0.ff.fc2', 'trf_blocks.15.att.v_proj', 'trf_blocks.13.ff.fc1', 'trf_blocks.25.att.v_proj', 'trf_blocks.16.att.o_proj', 'trf_blocks.23.att.k_proj', 'trf_blocks.5.att.v_proj', 'trf_blocks.8.ff.fc3', 'trf_blocks.12.ff.fc2', 'trf_blocks.1.att.k_proj', 'trf_blocks.19.ff.fc1', 'trf_blocks.24.ff.fc2', 'trf_blocks.1.ff.fc2', 'trf_blocks.14.ff.fc3', 'trf_blocks.1.ff.fc1', 'trf_blocks.22.att.o_proj', 'trf_blocks.26.ff.fc1', 'trf_blocks.12.att.o_proj', 'trf_blocks.15.att.k_proj', 'trf_blocks.19.ff.fc3', 'trf_blocks.11.att.k_proj', 'trf_blocks.19.ff.fc2', 'trf_blocks.20.att.k_proj', 'trf_blocks.6.att.v_proj', 'trf_blocks.18.ff.fc2', 'trf_blocks.18.att.q_proj', 'trf_blocks.15.att.q_proj', 'trf_blocks.3.att.o_proj', 'trf_blocks.22.att.k_proj', 'trf_blocks.0.att.q_proj', 'trf_blocks.2.att.v_proj', 'trf_blocks.20.ff.fc2', 'trf_blocks.25.att.o_proj', 'trf_blocks.24.ff.fc3', 'trf_blocks.26.ff.fc2', 'trf_blocks.21.att.v_proj', 'trf_blocks.6.ff.fc2', 'trf_blocks.7.att.k_proj', 'trf_blocks.8.att.k_proj', 'trf_blocks.15.att.o_proj', 'trf_blocks.22.att.q_proj', 'trf_blocks.27.att.k_proj', 'trf_blocks.10.att.v_proj', 'trf_blocks.22.att.v_proj', 'trf_blocks.14.att.v_proj', 'trf_blocks.9.att.k_proj', 'trf_blocks.11.att.v_proj', 'trf_blocks.3.att.q_proj', 'trf_blocks.6.att.k_proj', 'trf_blocks.9.ff.fc2', 'trf_blocks.23.ff.fc3', 'trf_blocks.20.att.q_proj', 'trf_blocks.8.ff.fc2', 'trf_blocks.11.ff.fc3', 'trf_blocks.17.ff.fc1', 'trf_blocks.13.ff.fc3', 'trf_blocks.26.att.v_proj', 'trf_blocks.18.att.o_proj', 'trf_blocks.5.ff.fc3', 'trf_blocks.12.ff.fc3', 'trf_blocks.24.att.k_proj', 'trf_blocks.4.att.o_proj', 'trf_blocks.26.att.q_proj', 'trf_blocks.17.att.o_proj', 'trf_blocks.2.ff.fc2', 'trf_blocks.17.ff.fc2', 'trf_blocks.16.att.v_proj', 'trf_blocks.14.att.k_proj', 'trf_blocks.20.att.o_proj', 'trf_blocks.5.att.q_proj', 'trf_blocks.7.att.q_proj', 'trf_blocks.27.ff.fc3', 'trf_blocks.9.att.q_proj', 'trf_blocks.10.ff.fc2', 'trf_blocks.4.att.k_proj', 'trf_blocks.17.ff.fc3', 'trf_blocks.17.att.k_proj', 'trf_blocks.8.att.o_proj', 'trf_blocks.15.ff.fc1', 'trf_blocks.18.att.k_proj', 'trf_blocks.6.ff.fc1', 'trf_blocks.3.ff.fc3', 'trf_blocks.25.att.k_proj', 'trf_blocks.21.att.k_proj', 'trf_blocks.25.ff.fc2', 'trf_blocks.12.att.k_proj', 'trf_blocks.4.ff.fc1', 'trf_blocks.12.att.v_proj', 'trf_blocks.1.att.o_proj', 'trf_blocks.2.att.o_proj', 'trf_blocks.14.att.o_proj', 'trf_blocks.20.ff.fc3', 'trf_blocks.10.ff.fc1', 'trf_blocks.21.att.q_proj', 'trf_blocks.19.att.q_proj', 'trf_blocks.27.att.v_proj', 'trf_blocks.0.ff.fc3', 'trf_blocks.11.att.q_proj', 'trf_blocks.22.ff.fc3', 'trf_blocks.1.att.v_proj', 'trf_blocks.24.att.o_proj', 'trf_blocks.11.ff.fc2', 'trf_blocks.7.ff.fc1', 'trf_blocks.22.ff.fc1', 'trf_blocks.9.att.v_proj', 'trf_blocks.19.att.k_proj', 'trf_blocks.3.att.v_proj', 'trf_blocks.7.att.o_proj', 'trf_blocks.11.att.o_proj', 'trf_blocks.16.att.k_proj', 'trf_blocks.21.ff.fc1', 'trf_blocks.2.att.q_proj', 'trf_blocks.21.ff.fc3', 'trf_blocks.6.ff.fc3', 'trf_blocks.9.ff.fc1', 'trf_blocks.27.ff.fc1', 'trf_blocks.5.ff.fc2', 'trf_blocks.6.att.o_proj', 'trf_blocks.18.ff.fc3', 'trf_blocks.20.ff.fc1', 'trf_blocks.14.ff.fc2', 'trf_blocks.18.att.v_proj', 'trf_blocks.26.att.o_proj', 'trf_blocks.10.ff.fc3']
trainable params: 24,313,856 || all params: 8,079,164,440 || trainable%: 0.3009
Model creation and weight loading complete.
Model structure created and weights loaded.
Gradient checkpointing enabled for Transformer blocks.
Setting up optimizer...
Optimizer: Base/LoRA LR=2e-05, Hopfield LR=0.0001
Found 392 trainable base/LoRA params, 8 trainable Hopfield params.
Using 8-bit AdamW optimizer (bitsandbytes).
Loading datasets...
Loading processed data for split 'train' from cache: ./data_cache/processed_narrativeqa/processed_train_6cca09ae25f51aa4.pt
Successfully loaded 32747 items from cache.
Loading processed data for split 'validation' from cache: ./data_cache/processed_narrativeqa/processed_validation_69e080a0a7e34564.pt
Successfully loaded 3461 items from cache.
Scheduler: Type=cosine, Total Steps=2047, Num Processes=1
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include/TH -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/pl217/miniconda3/envs/simpler_env/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include/TH -isystem /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/pl217/miniconda3/envs/simpler_env/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Time to load cpu_adam op: 35.037761926651 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.001000, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=0
[2025-04-20 14:10:49,922] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown
[2025-04-20 14:10:49,922] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 1
Warning: The cache directory for DeepSpeed Triton autotune, /home/pl217/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
