/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/pl217/continualearning/model.py:636: UserWarning: Weight loading missing non-Hopfield keys: ['out_head.weight']
  if missing: warnings.warn(f"Weight loading missing non-Hopfield keys: {missing}")
/home/pl217/continualearning/model.py:266: UserWarning: Error assigning sampled embeddings (shape mismatch?): shape '[24, 256, 128]' is invalid for input of size 18874368. Falling back to random.
  warnings.warn(f"Error assigning sampled embeddings (shape mismatch?): {e}. Falling back to random.")
/home/pl217/continualearning/data_loader.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self._processed_data = torch.load(self.cache_path)
Using /home/pl217/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Creating extension directory /home/pl217/.cache/torch_extensions/py311_cu124/cpu_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/pl217/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...
/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/pl217/continualearning/train.py", line 526, in <module>
[rank0]:     train(config_file)
[rank0]:   File "/home/pl217/continualearning/train.py", line 317, in train
[rank0]:     model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
[rank0]:                                                                         ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/accelerate/accelerator.py", line 1440, in prepare
[rank0]:     result = self._prepare_deepspeed(*args)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/accelerate/accelerator.py", line 2033, in _prepare_deepspeed
[rank0]:     engine, optimizer, _, lr_scheduler = ds_initialize(**kwargs)
[rank0]:                                          ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/deepspeed/__init__.py", line 193, in initialize
[rank0]:     engine = DeepSpeedEngine(args=args,
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 278, in __init__
[rank0]:     self._configure_distributed_model(model)
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1260, in _configure_distributed_model
[rank0]:     self.module.to(self.device)
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1340, in to
[rank0]:     return self._apply(convert)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 900, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 900, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 900, in _apply
[rank0]:     module._apply(fn)
[rank0]:   [Previous line repeated 4 more times]
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 927, in _apply
[rank0]:     param_applied = fn(param)
[rank0]:                     ^^^^^^^^^
[rank0]:   File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1326, in convert
[rank0]:     return t.to(
[rank0]:            ^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 40.50 MiB is free. Process 2628526 has 38.08 GiB memory in use. Including non-PyTorch memory, this process has 6.39 GiB memory in use. Of the allocated memory 5.94 GiB is allocated by PyTorch, and 46.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W420 14:10:59.045992500 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
E0420 14:11:00.096000 2840932 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2841038) of binary: /home/pl217/miniconda3/envs/simpler_env/bin/python3.11
Traceback (most recent call last):
  File "/home/pl217/miniconda3/envs/simpler_env/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-20_14:11:00
  host      : node5.athena
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2841038)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
