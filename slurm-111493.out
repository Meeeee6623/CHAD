[2025-04-21 22:41:42,535] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/pl217/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
--- Configuration ---
model_name: meta-llama/Llama-3.2-3B-Instruct
model_dtype: bfloat16
attn_implementation: flash_attention_2
max_eval_examples: 5
emb_dim: 3072
n_layers: 28
n_heads: 24
n_kv_groups: 8
hidden_dim: 8192
rope_base: 500000.0
rms_norm_eps: 1e-05
vocab_size: 128256
original_context_length: 128000
context_length: 4096
rope_freq: None
hopfield_heads: 24
hopfield_memory_slots: 512
hopfield_num_updates: 2
hopfield_init_method: embedding_sampling
hopfield_update_strategy: gated
hopfield_memory_update_lr: 0.018
hopfield_gate_input_pooling: attention
hopfield_update_target_method: max_query
hopfield_clamp_patterns: 3.0
hopfield_clamp_beta: 6.0
hopfield_combine_method: add
dataset_name: deepmind/narrativeqa
story_chunk_size: 768
story_chunk_overlap: 128
max_answer_length: 64
data_cache_dir: ./data_cache
output_dir: ./narrativeqa_hat_finetune_3.2_3B_run2
num_train_epochs: 20
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 64
learning_rate: 3e-05
hopfield_lr_multiplier: 5.0
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-08
max_grad_norm: 1.0
lr_scheduler_type: cosine
warmup_ratio: 0.08
logging_steps: 20
save_steps: 900
eval_steps: 900
seed: 42
gradient_checkpointing: True
use_lora: True
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
num_beams: 1
---------------------
---> TensorBoard logging initialized. Log directory for this run: ./narrativeqa_hat_finetune_3.2_3B_run2/narrativeqa_hat
Loading tokenizer...
No PAD token found. Adding and using EOS token as PAD.
Tokenizer loaded. Vocab size: 128000
BOS: '<|begin_of_text|>' (ID: 128000), EOS: '<|eot_id|>' (ID: 128009), PAD: '<|eot_id|>' (ID: 128009)
Creating/Loading model and weights...
Initializing HopfieldLlama3Model (Scratch Base) with dtype: torch.bfloat16
Gradient checkpointing enabled for Transformer blocks.
Loading pretrained weights for meta-llama/Llama-3.2-3B-Instruct...
Assuming 2 weight shards for meta-llama/Llama-3.2-3B-Instruct.
Loading weights from model-00001-of-00002.safetensors to CPU...
Loading weights from model-00002-of-00002.safetensors to CPU...
Base model weights loaded into HAT structure.
Applying weight tying for output head.
