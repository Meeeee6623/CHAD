2025-04-21 03:38:49.711329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-21 03:38:49.730567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1745221129.754734 2936441 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1745221129.761953 2936441 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1745221129.780937 2936441 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1745221129.780970 2936441 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1745221129.780978 2936441 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1745221129.780984 2936441 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-21 03:38:49.786328: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/pl217/continualearning/model.py:725: UserWarning: Weight loading missing non-Hopfield keys: ['out_head.weight']
  if missing: warnings.warn(f"Weight loading missing non-Hopfield keys: {missing}")
Inference:   0%|          | 0/6 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Inference:   0%|          | 0/6 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/peft/peft_model.py", line 793, in __getattr__
    return super().__getattr__(name)  # defer to nn.Module's logic
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'PeftModelForCausalLM' object has no attribute 'generation_config'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/peft/tuners/lora/model.py", line 359, in __getattr__
    return super().__getattr__(name)  # defer to nn.Module's logic
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'LoraModel' object has no attribute 'generation_config'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/pl217/continualearning/inference.py", line 248, in <module>
    run_inference(args.config, args.checkpoint, args.num_examples) 
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pl217/continualearning/inference.py", line 186, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/peft/peft_model.py", line 1868, in generate
    self.base_model.model.generation_config = self.generation_config
                                              ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/peft/peft_model.py", line 797, in __getattr__
    return getattr(self.base_model, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/peft/tuners/lora/model.py", line 363, in __getattr__
    return getattr(self.model, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pl217/miniconda3/envs/simpler_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'HopfieldLlama3Model' object has no attribute 'generation_config'
