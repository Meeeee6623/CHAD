[2025-04-21 04:16:42,359] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/pl217/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Using device: cuda
Using model dtype: torch.bfloat16
Loading tokenizer...
No PAD token found. Adding and using EOS token as PAD.
Tokenizer loaded. Vocab size: 128000
BOS: '<|begin_of_text|>' (ID: 128000), EOS: '<|eot_id|>' (ID: 128009), PAD: '<|eot_id|>' (ID: 128009)
Creating base model structure...
Initialized Gated Update mechanism.
Gradient checkpointing enabled for Transformer blocks.
Base model structure created.
Loading pretrained weights for meta-llama/Llama-3.2-3B-Instruct into base structure...
Initializing HopfieldLlama3Model (Scratch Base) with dtype: torch.bfloat16
Initialized Gated Update mechanism.
Gradient checkpointing enabled for Transformer blocks.
Loading pretrained weights for meta-llama/Llama-3.2-3B-Instruct...
Assuming 2 weight shards for meta-llama/Llama-3.2-3B-Instruct.
Loading weights from model-00001-of-00002.safetensors to CPU...
Loading weights from model-00002-of-00002.safetensors to CPU...
Base model weights loaded into HAT structure.
Applying weight tying for output head.
Explicitly initializing Hopfield memory...
Initializing Hopfield memory (embedding_sampling)...
Initialized stored patterns by sampling embeddings (sliced to head_dim).
Model creation and weight loading complete.
Pretrained weights loaded into base model.
Loading LoRA adapter from checkpoint: /home/pl217/continualearning/narrativeqa_hat_finetune_3.2_3B_run1/checkpoint-10
LoRA adapter loaded.
Model moved to device and set to eval mode.
Generation config attached to model.
Loading test dataset (summaries)...
Loading processed summary data for split 'test' from cache: ./data_cache/processed_narrativeqa_summary/processed_summary_test_f57db746d7d48033.pt
Successfully loaded 10557 items from cache.
Test dataset loaded with 10557 examples.
Running inference on first 10 examples...
Hopfield memory state cleared.
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Initializing RoPE/Mask buffers (len=4096) on device cuda:0
Inference job finished.
