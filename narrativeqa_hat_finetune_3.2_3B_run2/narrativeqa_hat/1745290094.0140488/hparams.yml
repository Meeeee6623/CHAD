adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-08
attn_implementation: flash_attention_2
context_length: 4096
data_cache_dir: ./data_cache
dataset_name: deepmind/narrativeqa
emb_dim: 3072
eval_steps: 900
gradient_accumulation_steps: 64
gradient_checkpointing: true
hidden_dim: 8192
hopfield_clamp_beta: 6.0
hopfield_clamp_patterns: 3.0
hopfield_combine_method: add
hopfield_gate_input_pooling: attention
hopfield_heads: 24
hopfield_init_method: embedding_sampling
hopfield_lr_multiplier: 5.0
hopfield_memory_slots: 512
hopfield_memory_update_lr: 0.018
hopfield_num_updates: 2
hopfield_update_strategy: gated
hopfield_update_target_method: max_query
learning_rate: 3.0e-05
logging_steps: 20
lora_alpha: 64
lora_dropout: 0.05
lora_r: 32
lora_target_modules: '[''q_proj'', ''k_proj'', ''v_proj'', ''o_proj'', ''gate_proj'',
  ''up_proj'', ''down_proj'']'
lr_scheduler_type: cosine
max_answer_length: 64
max_eval_examples: 5
max_grad_norm: 1.0
model_dtype: bfloat16
model_name: meta-llama/Llama-3.2-3B-Instruct
n_heads: 24
n_kv_groups: 8
n_layers: 28
num_beams: 1
num_train_epochs: 20
original_context_length: 128000
output_dir: ./narrativeqa_hat_finetune_3.2_3B_run2
per_device_eval_batch_size: 1
per_device_train_batch_size: 1
rms_norm_eps: 1.0e-05
rope_base: 500000.0
rope_freq: None
save_steps: 900
seed: 42
story_chunk_overlap: 128
story_chunk_size: 768
use_lora: true
vocab_size: 128256
warmup_ratio: 0.08
weight_decay: 0.01
